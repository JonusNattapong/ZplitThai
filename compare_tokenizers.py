#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Compare and benchmark different Thai tokenizer versions
‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏±‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û tokenizer ‡∏£‡∏∏‡πà‡∏ô‡∏ï‡πà‡∏≤‡∏á‡πÜ
"""

import json
from pathlib import Path
from tokenizers import Tokenizer
from transformers import AutoTokenizer
import time
import sys

def load_tokenizers():
    """Load all available tokenizer versions"""
    tokenizers = {}
    
    # Load Advanced V2
    adv_v2_path = Path("AdvancedThaiTokenizerV2/tokenizer.json")
    if adv_v2_path.exists():
        tokenizers["Advanced V2"] = Tokenizer.from_file(str(adv_v2_path))
        print("‚úÖ Loaded Advanced V2")
    
    # Load Final Thai Tokenizer
    final_path = Path("FinalThaiTokenizer")
    if final_path.exists():
        try:
            tokenizers["Final"] = AutoTokenizer.from_pretrained(str(final_path))
            print("‚úÖ Loaded Final Thai Tokenizer")
        except Exception as e:
            print(f"‚ùå Failed to load Final: {e}")
    
    # Load Fixed V2
    fixed_v2_path = Path("FixedThaiTokenizerV2")
    if fixed_v2_path.exists():
        try:
            tokenizers["Fixed V2"] = AutoTokenizer.from_pretrained(str(fixed_v2_path))
            print("‚úÖ Loaded Fixed V2")
        except Exception as e:
            print(f"‚ùå Failed to load Fixed V2: {e}")
    
    return tokenizers

def test_real_world_content():
    """Test with real-world mixed Thai-English content"""
    test_cases = [
        # Social media posts
        "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏õ Starbucks ‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏î‡∏∑‡πà‡∏° Green Tea Latte ‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å! üòä #coffee #thailand",
        
        # News headlines
        "Apple ‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏±‡∏ß iPhone 15 Pro Max ‡∏£‡∏≤‡∏Ñ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô 44,900 ‡∏ö‡∏≤‡∏ó ‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏ó‡∏¢ 22 ‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô 2567",
        
        # Business communication
        "Meeting ‡∏ß‡∏±‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡∏£‡πå ‡πÄ‡∏ß‡∏•‡∏≤ 14:00 ‡∏ô. ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏° A ‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Q4 Strategy ‡πÅ‡∏•‡∏∞ Budget 2025",
        
        # Technical documentation
        "‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python 3.11 ‡∏ö‡∏ô Windows 10/11 ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ pip install pandas numpy matplotlib",
        
        # Food reviews
        "‡∏£‡πâ‡∏≤‡∏ô McDonald's ‡∏™‡∏≤‡∏Ç‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà Siam Paragon ‡∏°‡∏µ Big Mac Set ‡∏£‡∏≤‡∏Ñ‡∏≤ 189 ‡∏ö‡∏≤‡∏ó ‡∏£‡∏™‡∏ä‡∏≤‡∏ï‡∏¥‡∏î‡∏µ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°",
        
        # Travel content
        "Flight TG123 ‡∏à‡∏≤‡∏Å Bangkok (BKK) ‡πÑ‡∏õ Tokyo (NRT) ‡πÄ‡∏ß‡∏•‡∏≤ 23:55 ‡∏ô. ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏ö‡∏¥‡∏ô 6 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á 30 ‡∏ô‡∏≤‡∏ó‡∏µ",
        
        # Academic text
        "‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ Machine Learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Natural Language Processing ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÇ‡∏î‡∏¢ Prof. ‡∏™‡∏°‡∏ä‡∏≤‡∏¢",
        
        # Gaming
        "‡πÄ‡∏•‡πà‡∏ô FIFA 24 ‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏ó‡∏µ‡∏° Real Madrid vs Barcelona ‡∏™‡∏Å‡∏≠‡∏£‡πå 3-2 ‡∏°‡∏±‡∏ô‡∏™‡πå‡∏°‡∏≤‡∏Å!",
        
        # Shopping
        "‡∏ã‡∏∑‡πâ‡∏≠ MacBook Pro 16\" M3 Max ‡∏£‡∏≤‡∏Ñ‡∏≤ 89,900 ‡∏ö‡∏≤‡∏ó ‡∏ú‡πà‡∏≠‡∏ô 0% 10 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ó‡∏µ‡πà Apple Store Central World",
        
        # Government
        "‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç ‡∏≠‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® COVID-19 ‡∏£‡∏∞‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÉ‡∏™‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏≠‡∏≠‡∏±‡∏î"
    ]
    
    return test_cases

def decode_tokens_v2(tokenizer, encoding):
    """Manual decoding for Advanced V2 tokenizer"""
    if hasattr(encoding, 'tokens'):
        # Filter out special tokens
        filtered_tokens = [token for token in encoding.tokens 
                          if not (token.startswith('<') and token.endswith('>'))]
        return "".join(filtered_tokens)
    return str(encoding)

def benchmark_tokenizer(name, tokenizer, test_cases):
    """Benchmark a tokenizer on test cases"""
    results = {
        "name": name,
        "total_time": 0,
        "cases": [],
        "stats": {
            "avg_tokens": 0,
            "avg_ratio": 0,
            "total_chars": 0,
            "total_tokens": 0
        }
    }
    
    print(f"\nüß™ Testing {name}...")
    print("=" * 50)
    
    start_time = time.time()
    
    for i, text in enumerate(test_cases, 1):
        case_start = time.time()
        
        try:
            if name == "Advanced V2":
                # Use tokenizers library
                encoding = tokenizer.encode(text)
                tokens = encoding.tokens
                decoded = decode_tokens_v2(tokenizer, encoding)
                token_count = len(tokens)
            else:
                # Use HuggingFace tokenizer
                encoding = tokenizer.encode(text, add_special_tokens=False)
                tokens = tokenizer.convert_ids_to_tokens(encoding)
                decoded = tokenizer.decode(encoding, skip_special_tokens=True)
                token_count = len(tokens)
            
            case_time = time.time() - case_start
            char_count = len(text)
            ratio = char_count / token_count if token_count > 0 else 0
            
            # Check for Thai encoding issues
            has_issues = any(char in decoded for char in ['√†¬∏', '√†¬π', '√†¬∫'])
            
            case_result = {
                "id": i,
                "text": text[:50] + "..." if len(text) > 50 else text,
                "char_count": char_count,
                "token_count": token_count,
                "ratio": ratio,
                "time": case_time,
                "has_encoding_issues": has_issues,
                "decoded_matches": decoded.strip() == text.strip()
            }
            
            results["cases"].append(case_result)
            results["stats"]["total_chars"] += char_count
            results["stats"]["total_tokens"] += token_count
            
            status = "‚úÖ" if not has_issues and decoded.strip() == text.strip() else "‚ùå"
            print(f"  {status} Case {i}: {char_count} chars ‚Üí {token_count} tokens (ratio: {ratio:.2f})")
            
        except Exception as e:
            print(f"  ‚ùå Case {i}: Error - {str(e)[:50]}")
            case_result = {
                "id": i,
                "text": text[:50] + "..." if len(text) > 50 else text,
                "error": str(e)
            }
            results["cases"].append(case_result)
    
    results["total_time"] = time.time() - start_time
    
    # Calculate averages
    valid_cases = [c for c in results["cases"] if "error" not in c]
    if valid_cases:
        results["stats"]["avg_tokens"] = results["stats"]["total_tokens"] / len(valid_cases)
        results["stats"]["avg_ratio"] = results["stats"]["total_chars"] / results["stats"]["total_tokens"]
        results["stats"]["success_rate"] = len([c for c in valid_cases if not c.get("has_encoding_issues", True)]) / len(valid_cases) * 100
    
    print(f"üìä {name} Summary:")
    print(f"   Time: {results['total_time']:.3f}s")
    print(f"   Avg tokens: {results['stats']['avg_tokens']:.1f}")
    print(f"   Compression ratio: {results['stats']['avg_ratio']:.2f} chars/token")
    print(f"   Success rate: {results['stats'].get('success_rate', 0):.1f}%")
    
    return results

def main():
    print("üî¨ Thai Tokenizer Comparison & Benchmark")
    print("=" * 60)
    
    # Load tokenizers
    tokenizers = load_tokenizers()
    if not tokenizers:
        print("‚ùå No tokenizers found!")
        return
    
    # Prepare test cases
    test_cases = test_real_world_content()
    print(f"üìù Testing with {len(test_cases)} real-world cases...")
    
    # Benchmark each tokenizer
    all_results = {}
    for name, tokenizer in tokenizers.items():
        try:
            results = benchmark_tokenizer(name, tokenizer, test_cases)
            all_results[name] = results
        except Exception as e:
            print(f"‚ùå Failed to benchmark {name}: {e}")
    
    # Compare results
    print("\nüèÜ Final Comparison")
    print("=" * 60)
    
    comparison_data = []
    for name, results in all_results.items():
        stats = results["stats"]
        comparison_data.append({
            "name": name,
            "avg_ratio": stats.get("avg_ratio", 0),
            "success_rate": stats.get("success_rate", 0),
            "time": results["total_time"]
        })
    
    # Sort by success rate, then by compression ratio
    comparison_data.sort(key=lambda x: (-x["success_rate"], -x["avg_ratio"]))
    
    print("Rank | Tokenizer     | Success% | Compression | Time")
    print("-" * 55)
    for i, data in enumerate(comparison_data, 1):
        print(f"{i:4d} | {data['name']:<12} | {data['success_rate']:7.1f}% | {data['avg_ratio']:10.2f} | {data['time']:.3f}s")
    
    # Save benchmark results
    output_file = "tokenizer_benchmark_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Results saved to {output_file}")
    
    # Winner announcement
    if comparison_data:
        winner = comparison_data[0]
        print(f"\nü•á Winner: {winner['name']}")
        print(f"   Success Rate: {winner['success_rate']:.1f}%")
        print(f"   Compression: {winner['avg_ratio']:.2f} chars/token")
        print(f"   Speed: {winner['time']:.3f}s")

if __name__ == "__main__":
    main()
