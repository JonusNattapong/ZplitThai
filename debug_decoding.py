#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Focused test to debug Thai tokenizer decoding issues
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from train_hf_tokenizer import TokenizerConfig, create_advanced_tokenizer
from tokenizers import Tokenizer

def debug_tokenizer_decoding():
    """Debug the specific decoding issues we're seeing"""
    
    print("üîç Debugging Thai Tokenizer Decoding...")
    print("=" * 50)
    
    # Create minimal config
    config = TokenizerConfig(
        vocab_size=1000,
        model_type="unigram",
        enable_byte_fallback=False,
        normalize_text=False,
        use_thai_pretokenizer=False
    )
    
    # Create tokenizer
    tokenizer, trainer = create_advanced_tokenizer(config)
    
    # Train on minimal data
    test_corpus = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏™", "‡∏Å", "123", "abc", "hello",
        "‡∏Å‡∏¥‡∏ô", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏≠‡∏£‡πà‡∏≠‡∏¢", "‡πÅ‡∏°‡πà", "‡πÅ‡∏°‡∏ß", "‡πÅ‡∏°‡∏•‡∏á",
        "‡πÄ‡∏î‡πá‡∏Å", "‡πÄ‡∏î‡∏¥‡∏ô", "‡πÄ‡∏î‡∏≤", "‡∏Å‡πà‡∏≠‡∏ô", "‡∏´‡∏•‡∏±‡∏á",
        " ", "  ", "‡∏Å‡∏¥‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß", "123 abc", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ 123"
    ]
    
    print("üèãÔ∏è Training on test corpus...")
    tokenizer.train_from_iterator(test_corpus, trainer)
    print("‚úÖ Training completed!")
    
    # Test cases that were failing
    test_cases = [
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ",           # Working
        "‡∏™",               # Working  
        "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì",           # Working
        "‡∏Å‡∏¥‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß ‡∏≠‡∏£‡πà‡∏≠‡∏¢",    # Failing - spaces lost
        "123 ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ abc",   # Failing - non-Thai lost
        "‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏•‡∏±‡∏á",        # Failing - spaces lost
    ]
    
    print("\nüß™ Testing problematic cases:")
    print()
    
    for i, text in enumerate(test_cases):
        print(f"Test {i+1}: '{text}'")
        
        try:
            # Encode
            encoding = tokenizer.encode(text)
            tokens = encoding.tokens
            token_ids = encoding.ids
            
            print(f"  Tokens: {tokens}")
            print(f"  Token IDs: {token_ids}")
            
            # Try different decoding methods
            decoded1 = tokenizer.decode(token_ids)
            decoded2 = tokenizer.decode(token_ids, skip_special_tokens=True)
            
            print(f"  Decoded (with special): '{decoded1}'")
            print(f"  Decoded (no special):   '{decoded2}'")
            
            # Check which one is better
            match1 = (decoded1.strip() == text.strip())
            match2 = (decoded2.strip() == text.strip())
            
            print(f"  Match with special: {match1}")
            print(f"  Match no special:   {match2}")
            
            if match1 or match2:
                print(f"  ‚úÖ SUCCESS (best: {'with special' if match1 else 'no special'})")
            else:
                print(f"  ‚ùå FAILED")
                
                # Debug: Show character-by-character comparison
                best_decoded = decoded2  # Usually no special tokens is better
                print(f"  Debug comparison:")
                print(f"    Original: {[c for c in text]}")
                print(f"    Decoded:  {[c for c in best_decoded]}")
                
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
        
        print()
    
    print("üîç Additional debugging:")
    print()
    
    # Check vocabulary
    vocab = tokenizer.get_vocab()
    print(f"Vocabulary size: {len(vocab)}")
    
    # Check special tokens
    special_tokens_in_vocab = [token for token in vocab if token.startswith('<') and token.endswith('>')]
    print(f"Special tokens: {special_tokens_in_vocab}")
    
    # Check space handling
    space_tokens = [token for token in vocab if ' ' in token]
    print(f"Space-containing tokens: {space_tokens[:10]}...")
    
    # Check Thai characters
    thai_tokens = [token for token in vocab if any('\u0e00' <= char <= '\u0e7f' for char in token)]
    print(f"Thai tokens: {len(thai_tokens)} (sample: {thai_tokens[:10]})")
    
    return True

if __name__ == "__main__":
    debug_tokenizer_decoding()
