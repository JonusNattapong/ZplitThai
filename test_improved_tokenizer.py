#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Focused test to fix Thai tokenizer issues
"""

import sys
import os
from pathlib import Path
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, normalizers, processors

def create_working_thai_tokenizer():
    """Create a working Thai tokenizer with proper configuration"""
    
    print("üîß Creating Improved Thai Tokenizer...")
    print("=" * 50)
    
    # Create a simple but effective configuration
    print("1Ô∏è‚É£ Setting up Unigram model...")
    model = models.Unigram()
    tokenizer = Tokenizer(model)
    
    # NO normalization - critical for Thai
    print("2Ô∏è‚É£ Disabling normalization...")
    tokenizer.normalizer = None
    
    # CRITICAL: Minimal pre-tokenization that doesn't interfere with spacing
    print("3Ô∏è‚É£ Setting up minimal pre-tokenization...")
    tokenizer.pre_tokenizer = pre_tokenizers.Punctuation()  # Only split on punctuation
    
    # CRITICAL: NO post-processor to avoid adding spaces between tokens
    print("4Ô∏è‚É£ Disabling post-processor...")
    tokenizer.post_processor = None
    
    # NO decoder - let it handle naturally
    print("5Ô∏è‚É£ Disabling decoder...")
    tokenizer.decoder = None
    
    # Create training data focused on Thai
    print("6Ô∏è‚É£ Creating Thai training data...")
    thai_training_data = [
        # Common Thai words and phrases
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞", "‡∏ú‡∏°", "‡∏î‡∏¥‡∏â‡∏±‡∏ô",
        "‡∏Å‡∏¥‡∏ô", "‡∏Ç‡πâ‡∏≤‡∏ß", "‡∏ô‡πâ‡∏≥", "‡∏≠‡∏£‡πà‡∏≠‡∏¢", "‡∏î‡∏µ", "‡∏°‡∏≤‡∏Å",
        "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ", "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô", "‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏®",
        "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡πÄ‡∏î‡∏¥‡∏ô", "‡∏ß‡∏¥‡πà‡∏á", "‡∏ô‡∏±‡πà‡∏á",
        "‡∏ö‡πâ‡∏≤‡∏ô", "‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏£‡πâ‡∏≤‡∏ô", "‡∏ï‡∏•‡∏≤‡∏î",
        "‡πÅ‡∏°‡πà", "‡∏û‡πà‡∏≠", "‡∏•‡∏π‡∏Å", "‡πÄ‡∏î‡πá‡∏Å", "‡∏Ñ‡∏ô",
        "‡∏£‡∏±‡∏Å", "‡∏ä‡∏≠‡∏ö", "‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏î‡∏π", "‡∏ü‡∏±‡∏á",
        "‡∏≠‡πà‡∏≤‡∏ô", "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô", "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô",
        "‡πÄ‡∏ß‡∏•‡∏≤", "‡∏ß‡∏±‡∏ô", "‡πÄ‡∏î‡∏∑‡∏≠‡∏ô", "‡∏õ‡∏µ",
        "‡∏ó‡∏µ‡πà", "‡∏à‡∏≤‡∏Å", "‡πÑ‡∏õ", "‡πÉ‡∏ô", "‡∏Å‡∏±‡∏ö",
        "‡πÅ‡∏•‡∏∞", "‡∏´‡∏£‡∏∑‡∏≠", "‡πÅ‡∏ï‡πà", "‡πÄ‡∏û‡∏£‡∏≤‡∏∞",
        # Thai with English/numbers
        "123 ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "Hello ‡∏Ñ‡∏£‡∏±‡∏ö", "Email: test@test.com",
        "‡∏£‡∏≤‡∏Ñ‡∏≤ 100 ‡∏ö‡∏≤‡∏ó", "‡πÄ‡∏ö‡∏≠‡∏£‡πå 02-123-4567",
        # Common combinations
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å", "‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å",
        "‡∏Å‡∏¥‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß", "‡πÑ‡∏õ‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "‡∏≠‡∏¢‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô",
        "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠", "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "‡πÄ‡∏•‡πà‡∏ô‡∏Å‡∏µ‡∏¨‡∏≤",
        # With spaces (important for Thai)
        "‡∏ú‡∏° ‡∏ä‡∏∑‡πà‡∏≠ ‡∏à‡∏≠‡∏´‡πå‡∏ô", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏≠‡∏≤‡∏Å‡∏≤‡∏® ‡∏î‡∏µ ‡∏°‡∏≤‡∏Å",
        "‡∏Å‡∏¥‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß ‡∏≠‡∏£‡πà‡∏≠‡∏¢", "‡πÑ‡∏õ ‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "‡∏≠‡∏¢‡∏π‡πà ‡∏ö‡πâ‡∏≤‡∏ô", "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠",
        # Edge cases
        "‡∏Å", "‡∏Ç", "‡∏Ñ", "‡∏á", "‡∏à",  # Single characters
        "‡∏Å‡∏Å", "‡∏Ç‡∏Ç", "‡∏Ñ‡∏Ñ",  # Doubled characters
        "‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏•‡∏±‡∏á", "‡πÄ‡∏î‡πá‡∏Å ‡πÄ‡∏î‡∏¥‡∏ô ‡πÄ‡∏î‡∏≤",
        "‡πÅ‡∏°‡πà ‡πÅ‡∏°‡∏ß ‡πÅ‡∏°‡∏•‡∏á",
    ]
    
    # Extend with more diverse examples
    extended_data = []
    for item in thai_training_data:
        extended_data.append(item)
        # Add variations
        extended_data.append(item + " ‡∏Ñ‡∏£‡∏±‡∏ö")
        extended_data.append(item + " ‡∏Ñ‡πà‡∏∞")
        if " " in item:
            extended_data.append(item.replace(" ", ""))  # No spaces version
    
    print(f"   Training with {len(extended_data)} examples")
    
    # Create trainer with minimal special tokens to avoid interference
    print("7Ô∏è‚É£ Setting up trainer...")
    trainer = trainers.UnigramTrainer(
        vocab_size=10000,  # Larger vocab for better subword learning
        special_tokens=["<unk>"],  # Only UNK token, no sentence markers
        show_progress=True,
        unk_token="<unk>"
    )
    
    # Train the tokenizer
    print("8Ô∏è‚É£ Training tokenizer...")
    try:
        tokenizer.train_from_iterator(extended_data, trainer)
        print("‚úÖ Training completed successfully!")
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        return None
    
    print(f"   Final vocabulary size: {len(tokenizer.get_vocab())}")
    
    return tokenizer

def test_tokenizer_comprehensive(tokenizer):
    """Test the tokenizer comprehensively"""
    
    print("\nüß™ Testing Tokenizer Performance...")
    print("=" * 50)
    
    test_cases = [
        # Basic Thai
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ",
        "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì", 
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö",
        
        # Thai with spaces
        "‡∏Å‡∏¥‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß ‡∏≠‡∏£‡πà‡∏≠‡∏¢",
        "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏≠‡∏≤‡∏Å‡∏≤‡∏® ‡∏î‡∏µ",
        "‡∏ú‡∏° ‡∏ä‡∏∑‡πà‡∏≠ ‡∏à‡∏≠‡∏´‡πå‡∏ô",
        
        # Mixed content
        "123 ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ abc",
        "Hello ‡∏Ñ‡∏£‡∏±‡∏ö",
        "Email: test@test.com ‡πÇ‡∏ó‡∏£ 02-123-4567",
        
        # Complex Thai
        "‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏•‡∏±‡∏á",
        "‡πÄ‡∏î‡πá‡∏Å ‡πÄ‡∏î‡∏¥‡∏ô ‡πÄ‡∏î‡∏≤",
        "‡πÅ‡∏°‡πà ‡πÅ‡∏°‡∏ß ‡πÅ‡∏°‡∏•‡∏á",
        
        # Edge cases
        "‡∏Å",
        "",
        " ",
        "12345",
        "abc",
    ]
    
    results = {'passed': 0, 'failed': 0, 'total': len(test_cases)}
    
    for i, text in enumerate(test_cases):
        try:
            # Encode
            encoding = tokenizer.encode(text)
            tokens = encoding.tokens
            
            # Decode
            decoded = tokenizer.decode(encoding.ids)
            
            # Check roundtrip
            success = (decoded.strip() == text.strip())
            
            if success:
                results['passed'] += 1
                status = "‚úÖ"
            else:
                results['failed'] += 1
                status = "‚ùå"
            
            print(f"{status} Test {i+1}: '{text}'")
            print(f"    Tokens ({len(tokens)}): {tokens}")
            print(f"    Decoded: '{decoded}'")
            print(f"    Roundtrip: {'PASS' if success else 'FAIL'}")
            print()
            
        except Exception as e:
            results['failed'] += 1
            print(f"‚ùå Test {i+1}: '{text}' -> ERROR: {e}")
            print()
    
    success_rate = results['passed'] / results['total'] * 100
    print(f"üìä Results: {results['passed']}/{results['total']} passed ({success_rate:.1f}%)")
    
    if success_rate >= 80:
        print("üéâ Tokenizer is working well!")
        return True
    else:
        print("‚ö†Ô∏è  Tokenizer needs improvement")
        return False

def main():
    """Main test function"""
    
    print("üöÄ Thai Tokenizer Fix Test")
    print("=" * 60)
    
    # Create improved tokenizer
    tokenizer = create_working_thai_tokenizer()
    
    if tokenizer is None:
        print("‚ùå Failed to create tokenizer")
        return False
    
    # Test the tokenizer
    success = test_tokenizer_comprehensive(tokenizer)
    
    if success:
        print("\nüéØ Saving working tokenizer...")
        try:
            Path("working_tokenizer").mkdir(exist_ok=True)
            tokenizer.save("working_tokenizer/tokenizer.json")
            print("‚úÖ Saved to working_tokenizer/tokenizer.json")
        except Exception as e:
            print(f"‚ùå Save failed: {e}")
    
    print("=" * 60)
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
