‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡∏Ç‡∏∂‡πâ‡∏ô Hugging Face ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‚úÖ ‡πÅ‡∏•‡∏∞ **‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ä‡∏£‡πå tokenizer ‡πÅ‡∏ö‡∏ö Rust-backend (`tokenizer.json`) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏ô Hugging Face Hub ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ**

---

## ‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face

### üîß ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß)

```
thai_tokenizer_hf/
‚îú‚îÄ‚îÄ tokenizer.json              ‚úÖ ‡∏ï‡∏±‡∏ß‡∏´‡∏•‡∏±‡∏Å
‚îú‚îÄ‚îÄ vocab.json                 ‚úÖ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPT2-type
‚îú‚îÄ‚îÄ merges.txt                 ‚úÖ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPT2-type
‚îú‚îÄ‚îÄ tokenizer_config.json      ‚úÖ ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ fast tokenizer
‚îî‚îÄ‚îÄ README.md (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°)    ‚úÖ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ tokenizer
```

---

## ü™Ñ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏≠‡∏±‡∏õ‡∏Ç‡∏∂‡πâ‡∏ô Hugging Face

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á CLI (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á)

```bash
pip install huggingface_hub
```

### 2. ‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô Hugging Face

```bash
huggingface-cli login
```

### 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á repo

```bash
huggingface-cli repo create thai-fast-tokenizer --type=tokenizer
```

> ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö: [https://huggingface.co/new/tokenizer](https://huggingface.co/new/tokenizer)

---

### 4. Upload ‡πÑ‡∏ü‡∏•‡πå

```bash
cd thai_tokenizer_hf

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ remote
git init
git remote add origin https://huggingface.co/username/thai-fast-tokenizer

# ‡πÉ‡∏™‡πà README.md ‡∏™‡∏±‡πâ‡∏ô ‡πÜ
echo "# Thai Fast Tokenizer using HuggingFace Rust Backend" > README.md

# commit ‡πÅ‡∏•‡∏∞ push
git add .
git commit -m "Add Thai tokenizer trained with HuggingFace tokenizers"
git push origin main
```

---

## üß™ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î

```python
from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast.from_pretrained("username/thai-fast-tokenizer")
tokens = tokenizer.tokenize("‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏´‡∏°‡∏π‡∏Å‡∏£‡∏≠‡∏ö‡∏≠‡∏£‡πà‡∏≠‡∏¢")
print(tokens)
```

---

## üìå ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

| ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥                        | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•                                                 |
| ---------------------------- | ------------------------------------------------------ |
| ‡πÉ‡∏™‡πà `README.md`              | ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô tokenizer                        |
| ‡πÄ‡∏û‡∏¥‡πà‡∏° `LICENSE`              | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ä‡πâ MIT ‡∏´‡∏£‡∏∑‡∏≠ Apache 2.0 ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏¥‡∏î‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå corpus |
| ‡∏£‡∏∞‡∏ö‡∏∏ `tokenizer_config.json` | ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö `AutoTokenizer` ‡πÑ‡∏î‡πâ                     |

---
